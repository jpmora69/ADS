{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# importing modules required for the optimization\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate some fake data.  \n",
    "# obs is number of observations.\n",
    "# params is the number of parameters to be estimated ignoring the constant.  \n",
    "# It is adjusted to include a constant using statsmodels \"add_constant\"\n",
    "\n",
    "obs = 2000\n",
    "params = 1\n",
    "\n",
    "params = params + 1\n",
    "beta = np.random.randn(params, 1)\n",
    "beta0 = np.zeros((params, 1))\n",
    "X = np.random.randn(obs, params-1)\n",
    "X = sm.add_constant(X)\n",
    "y = np.dot(X, beta) + np.random.randn(obs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 2047.839620\n",
      "         Iterations: 5\n",
      "         Function evaluations: 10\n",
      "         Gradient evaluations: 10\n"
     ]
    }
   ],
   "source": [
    "# This is least squares using BFGS to recover the covariance matrix.  \n",
    "# b is beta.  y and X are endog and exog respectively passed to the function.\n",
    "# Scratch space for beta is necessary.\n",
    "\n",
    "def func(b, y, X, obs, params):\n",
    "    bv = b.view()\n",
    "    bv.shape = params, 1\n",
    "    e = y - np.dot(X, bv)\n",
    "    return np.array(np.sum(e**2))\n",
    "\n",
    "def func_grad(b, y, X, obs, params):\n",
    "    bv = b.view()\n",
    "    bv.shape = params, 1\n",
    "    foc = -np.sum(X * (y - np.dot(X, bv)), axis=0)\n",
    "    return np.array(foc)\n",
    "\n",
    "res = minimize(func, beta0, args=(y, X, obs, params), method='BFGS', \n",
    "               jac=func_grad, options={'disp': True, 'maxiter':1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configure the matrices for the variance and the error\n",
    "\n",
    "betahat = res.x.reshape((params, 1))\n",
    "e = y - np.dot(X, betahat)\n",
    "s2 = np.dot(np.transpose(e), e)/(obs - params)\n",
    "cov = s2*res.hess_inv\n",
    "\n",
    "se, t = np.zeros((params, 1)), np.zeros((params, 1))\n",
    "\n",
    "for i in range(0, params):\n",
    "    se[i] = np.sqrt(cov[i,i])\n",
    "    t[i] = res.x[i]/np.sqrt(cov[i,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.31350182]\n",
      " [ 0.89056753]]\n",
      "[[ 0.34729575]\n",
      " [ 0.87543959]]\n",
      "[[ 0.0226411 ]\n",
      " [ 0.02265815]]\n",
      "[[ 15.33917207]\n",
      " [ 38.63685554]]\n"
     ]
    }
   ],
   "source": [
    "# Display beta from the DGP, betahats, their standard errors, and their t-statistics.\n",
    "\n",
    "print beta\n",
    "print betahat\n",
    "print se\n",
    "print t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate some fake data.  \n",
    "# obs is number of observations.\n",
    "# params is the number of parameters to be estimated ignoring the constant.  \n",
    "# It is adjusted to include a constant using statsmodels \"add_constant\"\n",
    "\n",
    "obs = 5000\n",
    "params = 1\n",
    "\n",
    "params = params + 1\n",
    "beta = np.random.randn(params, 1)\n",
    "beta0 = np.zeros((params, 1))\n",
    "X = np.random.randn(obs, params-1)\n",
    "X = sm.add_constant(X)\n",
    "y = np.dot(X, beta) + np.random.randn(obs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 4947.025424\n",
      "         Iterations: 5\n",
      "         Function evaluations: 12\n",
      "         Gradient evaluations: 12\n"
     ]
    }
   ],
   "source": [
    "# This is least squares using BFGS to recover the covariance matrix.  \n",
    "# b is beta.  y and X are endog and exog respectively passed to the function.\n",
    "# Scratch space for beta is necessary.\n",
    "\n",
    "def func(b, y, X, obs, params):\n",
    "    bv = b.view()\n",
    "    bv.shape = params, 1\n",
    "    e = y - np.dot(X, bv)\n",
    "    return np.array(np.sum(e**2))\n",
    "\n",
    "def func_grad(b, y, X, obs, params):\n",
    "    bv = b.view()\n",
    "    bv.shape = params, 1\n",
    "    foc = -np.sum(X * (y - np.dot(X, bv)), axis=0)\n",
    "    return np.array(foc)\n",
    "\n",
    "res = minimize(func, beta0, args=(y, X, obs, params), method='BFGS', \n",
    "               jac=func_grad, options={'disp': True, 'maxiter':1000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# configure the matrices for the variance and the error\n",
    "\n",
    "betahat = res.x.reshape((params, 1))\n",
    "e = y - np.dot(X, betahat)\n",
    "s2 = np.dot(np.transpose(e), e)/(obs - params)\n",
    "cov = s2*res.hess_inv\n",
    "\n",
    "se, t = np.zeros((params, 1)), np.zeros((params, 1))\n",
    "\n",
    "for i in range(0, params):\n",
    "    se[i] = np.sqrt(cov[i,i])\n",
    "    t[i] = res.x[i]/np.sqrt(cov[i,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.01448344]\n",
      " [ 0.74906768]]\n",
      "[[ 0.99853222]\n",
      " [ 0.773718  ]]\n",
      "[[ 0.01407067]\n",
      " [ 0.01401307]]\n",
      "[[ 70.96551466]\n",
      " [ 55.21400972]]\n"
     ]
    }
   ],
   "source": [
    "print beta\n",
    "print betahat\n",
    "print se\n",
    "print t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, when we change the number of observations for the sample in the function (2000 aand 5000 vs 1000 initially), we get higher levels of the t-statistic, therefore demostrating that the estimated mean of the observated values lies far away from the mean of the population the model that is trying to explain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
